#import matplotlib.pyplot as plt
#import matplotlib.image as mpimg
import sklearn as sk
from sklearn.datasets import load_digits
import numpy as np
import random

X = load_digits().images
Y = load_digits().target

TrainingI, TestI, TrainingT, TestT = sk.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)

mask = np.isin(TrainingT, [1,5,7])
testmask = np.isin(TestT, [1,5,7])

#print(TrainingT[1])
#print(TestT[2])
"""
#Maske fÃ¼r unseren Datensatz
mask = [] #hier wird die maske definiert
for i in range(len(digits.target)-450):
    if digits.target[i] == 1:
        mask.append(i)
    elif digits.target[i] == 5:
        mask.append(i)
    elif digits.target[i] == 7:
        mask.append(i)

testmask = [] #hier wird die maske definiert
for i in range(len(digits.target)-450, len(digits.target)):
    if digits.target[i] == 1:
        testmask.append(i)
    elif digits.target[i] == 5:
        testmask.append(i)
    elif digits.target[i] == 7:
        testmask.append(i)
"""

#One-Hot-Funktion
def one_hot(data):
    if data == 1:
        return np.array([[1],[0],[0]])
    elif data == 5:
        return np.array([[0],[1],[0]])
    elif data == 7:
        return np.array([[0],[0],[1]])

x = 1
y = -1
# Macht Theta mit random Werten zwischen 0 und 1
W1 = np.random.uniform(y , x, (64, 64))
b1 = np.random.uniform(y , x, (64, 1))
W2 = np.random.uniform(y , x, (32, 64))
b2 = np.random.uniform(y , x, (32, 1))
W3 = np.random.uniform(y , x, (3, 32))
b3 = np.random.uniform(y , x, (3, 1))

Theta = [W1, b1, W2, b2, W3, b3]

# Definition der Sigma Funktionen
def softplus(X):
    L = np.empty((len(X), 1))
    for i in range(len(X)):
        L[i][0] = np.log1p(np.exp(X[i][0]))
    return L


def sigmoid(X):
    L = np.empty((len(X), 1))
    for i in range(len(X)):
        L[i][0] = 1 / (1 + np.exp(-X[i][0]))
    return L


# Ableitungen der Sigma Funktionen
def d_softplus(X):
    return sigmoid(X)


def d_sigmoid(X):
    L = np.empty((len(X), 1))
    for i in range(len(X)):
        L[i][0] = np.exp(-X[i][0]) / ((1 + np.exp(-X[i][0])) ** 2)
    return L


def vec(A):
    L = []
    for i in range(0, len(A[0])):
        for j in range(0, len(A)):
            L.append([A[j][i]/50]) #Damit die zwischen 0 und 1 sind.
    return np.asarray(L)


def zunda(theta, x, sigma):
    z1 = np.matmul(theta[0], x) + theta[1]
    a1 = functionDecider(sigma, z1)
    z2 = np.matmul(theta[2], a1) + theta[3]
    a2 = functionDecider(sigma, z2)
    z3 = np.matmul(theta[4], a2) + theta[5]
    #a3 = functionDecider(sigma, z3) brauchen wir glaub ich nicht
    return [z1, a1, z2, a2, z3]

def functionDecider(func, X):
    if func == "sp":
        return softplus(X)
    if func == "spD":
        return d_softplus(X)
    if func == "sm":
        return sigmoid(X)
    if func == "smD":
        return d_sigmoid(X)

# Auswertung des neuronalen Netzes mit beliebigem theta
def forward(theta, x, sigma):
    return np.matmul(theta[4], functionDecider(sigma, np.matmul(theta[2], functionDecider(sigma, np.matmul(theta[0], x) + theta[1])) + theta[3])) + theta[5] #Maybe ohne sigma

# Definitionen der beiden l funktionen
def l1(S, Y):
    N = 0
    for i in range(len(S)):
        N += (S[i][0] - Y[i][0]) ** 2
    return N


def l2(S, Y):
    N = 0
    for i in range(len(S)):
        N += Y[i][0] * np.log1p(softmax(i, S))
    return -N


# Berechnet den Gradient von f(theta, x) abgeleitet nach dem Baies b^(l)
def gradb(l, theta, x, gradTf, sigma, z):
    # diag(D_sigma(z)) * W^T * b preparen

    z = functionDecider(sigma + "D", z).T  # transponieren, damit man dann als Liste zugreifen kann
    z = np.diag(z[0])

    # Mit den indizes gucken
    W = theta[2 * l].T
    b = gradTf[2 * l + 1]

    return np.matmul(np.matmul(z, W), b)  # multipliziert diag(D_sigma(z)) * W^T * b


# Berechnet den Gradienten von l(f(theta, x), y) abgeleitet nach der Weight matrix W^(l)
def gradW(l, gradTf, a):
    b = gradTf[2*l-1]
    a = a.T
    return np.matmul(b, a)


# Berechnet den Gradient von l(f(theta, x), y) abgeleitet nach theta. ell ist der Gradient von l
def gradTf(theta, x, y, sigma, ell):
    o = np.asarray([])  # ist ein Platzhalter damit beim hinzufuegen die Indizes sich nicht verschieben
    az = zunda(theta, x, sigma)

    f = az[4]

    w3 = np.matmul(ellDecider(ell, y, f).T, az[3].T)  # Schon zur komponente von grad theta l
    b3 = np.eye(3, 3)

    L = [o, o, o, o, w3, b3]
    L[3] = gradb(2, theta, x, L, sigma, az[2])
    L[1] = gradb(1, theta, x, L, sigma, az[0])

    L[1] = np.matmul(ellDecider(ell, y, f), L[1].T).T
    L[3] = np.matmul(ellDecider(ell, y, f), L[3].T).T
    L[5] = np.matmul(ellDecider(ell, y, f), L[5].T).T

    L[2] = gradW(2, L, az[1])
    L[0] = gradW(1, L, x)

    return L


def ellDecider(func, y, f):
    if func == "l1":
        return l1(f, y)
    if func == "Dl1":
        return gradl1(y, f)
    if func == "l2":
        return l2(f, y)
    if func == "Dl2":
        return gradl2(y, f)

# Ableitung wenn l gleich die Eukliedischen-Norm ist
def gradl1(y, f):
    return np.asarray([[2 * (f[0][0] - y[0][0]), 2 * (f[1][0] - y[1][0]), 2 * (f[2][0] - y[2][0])]])


# Berechnet die ableitung wenn l gleich der empirische log-Entropie Funktion ist
def gradl2(y, f):
    # Berechne zuerst die einzelnen Komponenten
    a = softmax(0, f)
    b = softmax(1, f)
    c = softmax(2, f)
    # Dann zusammen in einen Vektor schreiben und das mit dem y
    return np.asarray([[-y[0][0] + a, -y[1][0] + b, -y[2][0] + c]])


# berechnet Softmax
def softmax(i, f):
    x = np.exp(f[i][0])

    b = 0
    for k in range(len(f)):
        b += np.exp(f[k][0])

    return x / b

#Berechnet den Gradienten der Loss Funktion nach Theta abgeleitet

def gradLoss(theta, sigma, ell):
    e = [np.zeros((64, 64)), np.zeros((64, 1)), np.zeros((32, 64)), np.zeros((32, 1)), np.zeros((3, 32)), np.zeros((3, 1))] #Das Ergebnis
    sa = random.sample(mask, 10)
    #print(sa)
    for i in sa:
        l = gradTf(theta, vec(digits.images[i]), one_hot(digits.target[i]), sigma, ell)
        m = len(sa)
        e[0] = np.add(e[0], l[0]) / m
        e[1] = np.add(e[1], l[1]) / m
        e[2] = np.add(e[2], l[2]) / m
        e[3] = np.add(e[3], l[3]) / m
        e[4] = np.add(e[4], l[4]) / m
        e[5] = np.add(e[5], l[5]) / m
    return e


#fuehrt das Gradienten Absteigsverfahren fuer eine Schrittweite a und i Iterationen aus
def graddescent(theta, a, n, sigma, ell):
    for j in range(n):
        gL = gradLoss(theta, sigma, ell)
        theta[0] = np.add(theta[0], - a * gL[0])
        theta[1] = np.add(theta[1], - a * gL[1])
        theta[2] = np.add(theta[2], - a * gL[2])
        theta[3] = np.add(theta[3], - a * gL[3])
        theta[4] = np.add(theta[4], - a * gL[4])
        theta[5] = np.add(theta[5], - a * gL[5])
        #print(f"Gradienteniteration: {j}")
    return theta

def main(stepsize, gradIter, Epochen, sigma, ell, n):
    theta = Theta
    c = 0
    for i in range(Epochen):
        theta = graddescent(theta, stepsize, gradIter, sigma, ell)
        c += 1
        print(f"\rEpoche: {c}", end="", flush=True)
    C_matrix = np.zeros((3,3))
    #print(theta)
    for i in range(n):
        #print(forward(theta,vec(digits.images[mask[i]]),sigma))
        guess = maxindex(forward(theta,vec(digits.images[testmask[i]]),sigma))
        actual = maxindex(one_hot(digits.target[testmask[i]]))
        #print(actual,guess)
        C_matrix[actual][guess] += 1
    print()
    print("Confusion-Matrix:")
    return C_matrix

def maxindex(A):
    if A[0][0] > A[1][0] and A[0][0] > A[2][0]:
        return 0
    elif A[1][0] > A[0][0] and A[1][0] > A[2][0]:
        return 1
    elif A[2][0] > A[1][0] and A[2][0] > A[0][0]:
        return 2

#sm und Dl1
print(main(0.1, 150, 150,"sm", "Dl1", len(testmask)))

#sp und Dl1
#print(main(0.03, 150, 150,"sp", "Dl1", len(testmask)))

#sm und Dl2
#print(main(0.1, 150, 150,"sm", "Dl2", len(testmask)))

#sp und Dl2
#print(main(0.5, 150, 150,"sp", "Dl2", len(testmask)))

"""
Bei x durch 50 Teilen
Theta: -1 bis 1
grad Iter: 150
Epochen: 150
Zeit: immer so 30 sekunden

Sigmoid und DL1: stepsize: 0.1  Fehler: 1 

Softplus und DL1: stepsize: 0.03  Fehler: 1 bis 3

Sigmoid und DL2: stepsize: 0.1  Fehler: 1 

Softplus und Dl2: stepsize: 0.01  Fehler: 1 bis 2
"""
