{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5522b55d-680c-4401-b3e1-443670861828",
   "metadata": {},
   "source": [
    "Importieren der notwendigen Packete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28e9df95-ec48-4b8f-8ef6-6a49456a0abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b33e27-f8cf-4f00-9093-94d2ae481697",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Hochladen des Datensatzes und Maske erstellen:\n",
    "\n",
    "Dies tun wir indem wir den Datensatz digits durchgehen, wobei wir die letzten 450 als Testdaten nehmen und den rest als Trainingsdaten. \n",
    "Anschliessend filtern wir mit einer maske jeweills die Test und Trainingsdatensaetze nach den Werten 1, 5 und 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87b417d8-de0e-4166-9a31-af3ac8887004",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "\n",
    "#Maske fÃ¼r fuer den Trainingsdatensatz\n",
    "mask = [] \n",
    "\n",
    "for i in range(len(digits.target)-450):\n",
    "    #Filtern nach den Eintraegen 1, 5 und 7\n",
    "    if digits.target[i] == 1:\n",
    "        mask.append(i)\n",
    "    elif digits.target[i] == 5:\n",
    "        mask.append(i)\n",
    "    elif digits.target[i] == 7:\n",
    "        mask.append(i)\n",
    "\n",
    "#Maske fuer den Testdatansatz\n",
    "testmask = [] \n",
    "for i in range(len(digits.target)-450, len(digits.target)):\n",
    "    #Filtern nach den Eintraegen 1, 5 und 7\n",
    "    if digits.target[i] == 1:\n",
    "        testmask.append(i)\n",
    "    elif digits.target[i] == 5:\n",
    "        testmask.append(i)\n",
    "    elif digits.target[i] == 7:\n",
    "        testmask.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ffa837-f4cf-4388-8dd0-9536ad2ae55c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Aufgabe 1.\n",
    "\n",
    "In diesem Absatz sind one_hot und unsere aktivierungsfunktionen Softplus und Sigmoid mit ihren Ableitungen definiert. \n",
    "\n",
    "Die one_hot funktion konvertiert die Targets, also die richtigen Zuordnungen unserer Bilder, in dreidimensionale Vektoren. Dies benoetigne wir, um den Abstand von der loesung die das Neuronale Netz ermittelt hat mit der tatsaechlichen Loesung zu vergleichen.\n",
    "\n",
    "Als Aktivierungsfunktionen, haben wir Sigmoid und Softplus implementiert. Dabei haben wir fuer mehr Stabilitaet np.log1p anstelle des normalen Logarithmus verwendet.\n",
    "\n",
    "Wir haben ebenfalls die Ableitungen der Aktiviertungsfunktionen implementiert.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "026c1da5-7be5-4d95-9657-41254fc92036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-Hot-Funktion\n",
    "def one_hot(data):\n",
    "    if data == 1:\n",
    "        return np.array([[1],[0],[0]])\n",
    "    elif data == 5:\n",
    "        return np.array([[0],[1],[0]])\n",
    "    elif data == 7:\n",
    "        return np.array([[0],[0],[1]])\n",
    "\n",
    "\n",
    "# Definition der Sigma Funktionen\n",
    "def softplus(X):\n",
    "    L = np.empty((len(X), 1))\n",
    "    for i in range(len(X)):\n",
    "        L[i][0] = np.log1p(np.exp(X[i][0]))\n",
    "    return L\n",
    "\n",
    "\n",
    "def sigmoid(X):\n",
    "    L = np.empty((len(X), 1))\n",
    "    for i in range(len(X)):\n",
    "        L[i][0] = 1 / (1 + np.exp(-X[i][0]))\n",
    "    return L\n",
    "\n",
    "\n",
    "# Ableitungen der Sigma Funktionen\n",
    "def d_softplus(X):\n",
    "    return sigmoid(X)\n",
    "\n",
    "\n",
    "def d_sigmoid(X):\n",
    "    L = np.empty((len(X), 1))\n",
    "    for i in range(len(X)):\n",
    "        L[i][0] = np.exp(-X[i][0]) / ((1 + np.exp(-X[i][0])) ** 2)\n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c3604b-0414-4323-90fd-af64239dd59f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Initialisierung von Theta und dem Bild x.\n",
    "\n",
    "Damit wir fuer unser Gradienten Abstiegsverfahren einen Startwert haben, waehlen wir uns ein Theta mit Zufaelligen Eintraegen. \n",
    "Diese Zufaelligen Eintraege werden Normalverteilt aus dem Intervall zwischen -1 und 1 gewaehlt. \n",
    "Dabei hat unser zum Start gewaehltes Theta selbstverstaendlich die Selben Dimensionen wie in der Aufgabenstellung beschrieben.\n",
    "\n",
    "Ausserdem muessen wir unser eingabebild Skalieren und Vektorisieren. Da das Bild x eingabewerte ungefaehr zwischen 1 und 20 hat reicht es aus durch 50 zu teilen, damit sichergestellt ist, dass sich die Eintraege des Vektors zwischen 0 und 1 befinden.\n",
    "\n",
    "Wir moechten, dass die Eintraege von Theta und der Matrix unseres Bildes, betragsmaessig kleiner als 1 sind, da sonst die Werte die wir unserer Aktivierungsfunktion geben zu gross sind und keine guten Ergebnisse geliefert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cf4fdaf-1b3e-4e58-8dfc-9fbb3aed1855",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1\n",
    "y = -1\n",
    "# Erstellt Theta mit random Werten zwischen -1 und 1\n",
    "W1 = np.random.uniform(y , x, (64, 64))\n",
    "b1 = np.random.uniform(y , x, (64, 1))\n",
    "W2 = np.random.uniform(y , x, (32, 64))\n",
    "b2 = np.random.uniform(y , x, (32, 1))\n",
    "W3 = np.random.uniform(y , x, (3, 32))\n",
    "b3 = np.random.uniform(y , x, (3, 1))\n",
    "\n",
    "Theta = [W1, b1, W2, b2, W3, b3]\n",
    "\n",
    "def vec(A):\n",
    "    L = []\n",
    "    for i in range(0, len(A[0])):\n",
    "        for j in range(0, len(A)):\n",
    "            L.append([A[j][i]/50]) #Damit die zwischen 0 und 1 sind.\n",
    "    return np.asarray(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3655e6e-f1ad-4ea2-b608-6bc1f2d264fb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98a38130-b22b-44eb-a51e-38f31f426927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoche: 150\n",
      "Confusion-Matrix:\n",
      "[[46.  0.  0.]\n",
      " [ 0. 45.  0.]\n",
      " [ 1.  0. 44.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nBei x durch 50 Teilen\\nTheta: -1 bis 1\\ngrad Iter: 150\\nEpochen: 150\\nZeit: immer so 30 sekunden\\n\\nSigmoid und DL1: stepsize: 0.1  Fehler: 1 \\n\\nSoftplus und DL1: stepsize: 0.03  Fehler: 1 bis 3\\n\\nSigmoid und DL2: stepsize: 0.1  Fehler: 1 \\n\\nSoftplus und Dl2: stepsize: 0.01  Fehler: 1 bis 2\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def zunda(theta, x, sigma):\n",
    "    z1 = np.matmul(theta[0], x) + theta[1]\n",
    "    a1 = functionDecider(sigma, z1)\n",
    "    z2 = np.matmul(theta[2], a1) + theta[3]\n",
    "    a2 = functionDecider(sigma, z2)\n",
    "    z3 = np.matmul(theta[4], a2) + theta[5]\n",
    "    #a3 = functionDecider(sigma, z3) brauchen wir glaub ich nicht\n",
    "    return [z1, a1, z2, a2, z3]\n",
    "\n",
    "def functionDecider(func, X):\n",
    "    if func == \"sp\":\n",
    "        return softplus(X)\n",
    "    if func == \"spD\":\n",
    "        return d_softplus(X)\n",
    "    if func == \"sm\":\n",
    "        return sigmoid(X)\n",
    "    if func == \"smD\":\n",
    "        return d_sigmoid(X)\n",
    "\n",
    "# Auswertung des neuronalen Netzes mit beliebigem theta\n",
    "def forward(theta, x, sigma):\n",
    "    return np.matmul(theta[4], functionDecider(sigma, np.matmul(theta[2], functionDecider(sigma, np.matmul(theta[0], x) + theta[1])) + theta[3])) + theta[5] #Maybe ohne sigma\n",
    "\n",
    "# Definitionen der beiden l funktionen\n",
    "def l1(S, Y):\n",
    "    N = 0\n",
    "    for i in range(len(S)):\n",
    "        N += (S[i][0] - Y[i][0]) ** 2\n",
    "    return N\n",
    "\n",
    "\n",
    "def l2(S, Y):\n",
    "    N = 0\n",
    "    for i in range(len(S)):\n",
    "        N += Y[i][0] * np.log1p(softmax(i, S))\n",
    "    return -N\n",
    "\n",
    "\n",
    "# Berechnet den Gradient von f(theta, x) abgeleitet nach dem Baies b^(l)\n",
    "def gradb(l, theta, x, gradTf, sigma, z):\n",
    "    # diag(D_sigma(z)) * W^T * b preparen\n",
    "\n",
    "    z = functionDecider(sigma + \"D\", z).T  # transponieren, damit man dann als Liste zugreifen kann\n",
    "    z = np.diag(z[0])\n",
    "\n",
    "    # Mit den indizes gucken\n",
    "    W = theta[2 * l].T\n",
    "    b = gradTf[2 * l + 1]\n",
    "\n",
    "    return np.matmul(np.matmul(z, W), b)  # multipliziert diag(D_sigma(z)) * W^T * b\n",
    "\n",
    "\n",
    "# Berechnet den Gradienten von l(f(theta, x), y) abgeleitet nach der Weight matrix W^(l)\n",
    "def gradW(l, gradTf, a):\n",
    "    b = gradTf[2*l-1]\n",
    "    a = a.T\n",
    "    return np.matmul(b, a)\n",
    "\n",
    "\n",
    "# Berechnet den Gradient von l(f(theta, x), y) abgeleitet nach theta. ell ist der Gradient von l\n",
    "def gradTf(theta, x, y, sigma, ell):\n",
    "    o = np.asarray([])  # ist ein Platzhalter damit beim hinzufuegen die Indizes sich nicht verschieben\n",
    "    az = zunda(theta, x, sigma)\n",
    "\n",
    "    f = az[4]\n",
    "\n",
    "    w3 = np.matmul(ellDecider(ell, y, f).T, az[3].T)  # Schon zur komponente von grad theta l\n",
    "    b3 = np.eye(3, 3)\n",
    "\n",
    "    L = [o, o, o, o, w3, b3]\n",
    "    L[3] = gradb(2, theta, x, L, sigma, az[2])\n",
    "    L[1] = gradb(1, theta, x, L, sigma, az[0])\n",
    "\n",
    "    L[1] = np.matmul(ellDecider(ell, y, f), L[1].T).T\n",
    "    L[3] = np.matmul(ellDecider(ell, y, f), L[3].T).T\n",
    "    L[5] = np.matmul(ellDecider(ell, y, f), L[5].T).T\n",
    "\n",
    "    L[2] = gradW(2, L, az[1])\n",
    "    L[0] = gradW(1, L, x)\n",
    "\n",
    "    return L\n",
    "\n",
    "\n",
    "def ellDecider(func, y, f):\n",
    "    if func == \"l1\":\n",
    "        return l1(f, y)\n",
    "    if func == \"Dl1\":\n",
    "        return gradl1(y, f)\n",
    "    if func == \"l2\":\n",
    "        return l2(f, y)\n",
    "    if func == \"Dl2\":\n",
    "        return gradl2(y, f)\n",
    "\n",
    "# Ableitung wenn l gleich die Eukliedischen-Norm ist\n",
    "def gradl1(y, f):\n",
    "    return np.asarray([[2 * (f[0][0] - y[0][0]), 2 * (f[1][0] - y[1][0]), 2 * (f[2][0] - y[2][0])]])\n",
    "\n",
    "\n",
    "# Berechnet die ableitung wenn l gleich der empirische log-Entropie Funktion ist\n",
    "def gradl2(y, f):\n",
    "    # Berechne zuerst die einzelnen Komponenten\n",
    "    a = softmax(0, f)\n",
    "    b = softmax(1, f)\n",
    "    c = softmax(2, f)\n",
    "    # Dann zusammen in einen Vektor schreiben und das mit dem y\n",
    "    return np.asarray([[-y[0][0] + a, -y[1][0] + b, -y[2][0] + c]])\n",
    "\n",
    "\n",
    "# berechnet Softmax\n",
    "def softmax(i, f):\n",
    "    x = np.exp(f[i][0])\n",
    "\n",
    "    b = 0\n",
    "    for k in range(len(f)):\n",
    "        b += np.exp(f[k][0])\n",
    "\n",
    "    return x / b\n",
    "\n",
    "#Berechnet den Gradienten der Loss Funktion nach Theta abgeleitet\n",
    "\n",
    "def gradLoss(theta, sigma, ell):\n",
    "    e = [np.zeros((64, 64)), np.zeros((64, 1)), np.zeros((32, 64)), np.zeros((32, 1)), np.zeros((3, 32)), np.zeros((3, 1))] #Das Ergebnis\n",
    "    sa = random.sample(mask, 10)\n",
    "    #print(sa)\n",
    "    for i in sa:\n",
    "        l = gradTf(theta, vec(digits.images[i]), one_hot(digits.target[i]), sigma, ell)\n",
    "        m = len(sa)\n",
    "        e[0] = np.add(e[0], l[0]) / m\n",
    "        e[1] = np.add(e[1], l[1]) / m\n",
    "        e[2] = np.add(e[2], l[2]) / m\n",
    "        e[3] = np.add(e[3], l[3]) / m\n",
    "        e[4] = np.add(e[4], l[4]) / m\n",
    "        e[5] = np.add(e[5], l[5]) / m\n",
    "    return e\n",
    "\n",
    "\n",
    "#fuehrt das Gradienten Absteigsverfahren fuer eine Schrittweite a und i Iterationen aus\n",
    "def graddescent(theta, a, n, sigma, ell):\n",
    "    for j in range(n):\n",
    "        gL = gradLoss(theta, sigma, ell)\n",
    "        theta[0] = np.add(theta[0], - a * gL[0])\n",
    "        theta[1] = np.add(theta[1], - a * gL[1])\n",
    "        theta[2] = np.add(theta[2], - a * gL[2])\n",
    "        theta[3] = np.add(theta[3], - a * gL[3])\n",
    "        theta[4] = np.add(theta[4], - a * gL[4])\n",
    "        theta[5] = np.add(theta[5], - a * gL[5])\n",
    "        #print(f\"Gradienteniteration: {j}\")\n",
    "    return theta\n",
    "\n",
    "def main(stepsize, gradIter, Epochen, sigma, ell, n):\n",
    "    theta = Theta\n",
    "    c = 0\n",
    "    for i in range(Epochen):\n",
    "        theta = graddescent(theta, stepsize, gradIter, sigma, ell)\n",
    "        c += 1\n",
    "        print(f\"\\rEpoche: {c}\", end=\"\", flush=True)\n",
    "    C_matrix = np.zeros((3,3))\n",
    "    #print(theta)\n",
    "    for i in range(n):\n",
    "        #print(forward(theta,vec(digits.images[mask[i]]),sigma))\n",
    "        guess = maxindex(forward(theta,vec(digits.images[testmask[i]]),sigma))\n",
    "        actual = maxindex(one_hot(digits.target[testmask[i]]))\n",
    "        #print(actual,guess)\n",
    "        C_matrix[actual][guess] += 1\n",
    "    print()\n",
    "    print(\"Confusion-Matrix:\")\n",
    "    return C_matrix\n",
    "\n",
    "def maxindex(A):\n",
    "    if A[0][0] > A[1][0] and A[0][0] > A[2][0]:\n",
    "        return 0\n",
    "    elif A[1][0] > A[0][0] and A[1][0] > A[2][0]:\n",
    "        return 1\n",
    "    elif A[2][0] > A[1][0] and A[2][0] > A[0][0]:\n",
    "        return 2\n",
    "\n",
    "#sm und Dl1\n",
    "print(main(0.1, 150, 150,\"sm\", \"Dl1\", len(testmask)))\n",
    "\n",
    "#sp und Dl1\n",
    "#print(main(0.03, 150, 150,\"sp\", \"Dl1\", len(testmask)))\n",
    "\n",
    "#sm und Dl2\n",
    "#print(main(0.1, 150, 150,\"sm\", \"Dl2\", len(testmask)))\n",
    "\n",
    "#sp und Dl2\n",
    "#print(main(0.5, 150, 150,\"sp\", \"Dl2\", len(testmask)))\n",
    "\n",
    "\"\"\"\n",
    "Bei x durch 50 Teilen\n",
    "Theta: -1 bis 1\n",
    "grad Iter: 150\n",
    "Epochen: 150\n",
    "Zeit: immer so 30 sekunden\n",
    "\n",
    "Sigmoid und DL1: stepsize: 0.1  Fehler: 1 \n",
    "\n",
    "Softplus und DL1: stepsize: 0.03  Fehler: 1 bis 3\n",
    "\n",
    "Sigmoid und DL2: stepsize: 0.1  Fehler: 1 \n",
    "\n",
    "Softplus und Dl2: stepsize: 0.01  Fehler: 1 bis 2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1bf257-db45-46bd-942b-d02c3e7ad5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
