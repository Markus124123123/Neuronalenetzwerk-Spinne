import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from sklearn.datasets import load_digits
import numpy as np
import scipy as scipy
import sklearn as sk
import random

digits = load_digits()

#Maske fÃ¼r unseren Datensatz
mask = [] #hier wird die maske definiert
for i in range(len(digits.target)):
    if digits.target[i] == 1:
        mask.append(i)
    elif digits.target[i] == 5:
        mask.append(i)
    elif digits.target[i] == 7:
        mask.append(i)

#One-Hot-Funktion
def one_hot(data):
    if type(data) == np.ndarray:
        for i in range(len(digits.images)):
            if (digits.images[i]==data).all():
                return one_hot(i)
    else:
        if data == 1:
            return np.array([[1],[0],[0]])
        elif data == 5:
            return np.array([[0],[1],[0]])
        elif data == 7:
            return np.array([[0],[0],[1]])
"""
#Alte Theta initialisierung
W1 = np.empty((64, 64))
b1 = np.empty((64, 1))
c = 1
for i in range(64):
    b1[i][0] = c/1000
    for j in range(64):
        W1[i][j] = c/1000
        c += 1
a = 1
W2 = np.empty((32, 64))
b2 = np.empty((32, 1))
for i in range(32):
    b2[i][0] = a/1000
    for j in range(64):
        W2[i][j] = a/1000
        a += 1
b = 1
W3 = np.empty((3, 32))
b3 = np.empty((3, 1))
for i in range(3):
    b3[i][0] = b/1000
    for j in range(32):
        W3[i][j] = b/1000
        b += 1
Theta = [W1, b1, W2, b2, W3, b3]
"""

x = 1
y = -1
# Macht Theta mit random Werten zwischen 0 und 1
W1 = np.random.uniform(y , x, (64, 64))
b1 = np.random.uniform(y , x, (64, 1))
W2 = np.random.uniform(y , x, (32, 64))
b2 = np.random.uniform(y , x, (32, 1))
W3 = np.random.uniform(y , x, (3, 32))
b3 = np.random.uniform(y , x, (3, 1))

Theta = [W1, b1, W2, b2, W3, b3]

#print(Theta)

# Definition der Sigma Funktionen
def softplus(X):
    L = np.empty((len(X), 1))
    for i in range(len(X)):
        L[i][0] = np.log1p(1 + scipy.special.expit(X[i][0]))
    return L


def sigmoid(X):
    L = np.empty((len(X), 1))
    for i in range(len(X)):
        L[i][0] = 1 / (1 + scipy.special.expit(-X[i][0]))
    return L


# Ableitungen der Sigma Funktionen
def d_softplus(X):
    return sigmoid(X)


def d_sigmoid(X):
    L = np.empty((len(X), 1))
    for i in range(len(X)):
        L[i][0] = scipy.special.expit(-X[i][0]) / ((1 + scipy.special.expit(-X[i][0])) ** 2)
    return L


def zt(theta, x, l, sigma):
    if l == 1:
        return np.matmul(theta[0], x) + theta[1]
    elif l == 2:
        return np.matmul(theta[2], at(theta, x, 1, sigma)) + theta[3]
    else:
        return np.matmul(theta[4], at(theta, x, 2, sigma)) + theta[5]


def at(theta, x, l, sigma):
    if l == 0:
        return x
    return functionDecider(sigma, zt(theta, x, l, sigma))

def functionDecider(func, X):
    if func == "sp":
        return softplus(X)
    if func == "spD":
        return d_softplus(X)
    if func == "sm":
        return sigmoid(X)
    if func == "smD":
        return d_sigmoid(X)

def vec(A):
    L = []
    for i in range(0, len(A[0])):
        for j in range(0, len(A)):
            L.append([A[j][i]/50]) #Damit die zwischen 0 und 1 sind.
    return np.asarray(L)

# Auswertung des neuronalen Netzes mit beliebigem theta
def forward(theta, x, sigma):
    return np.matmul(theta[4], functionDecider(sigma, np.matmul(theta[2], functionDecider(sigma, np.matmul(theta[0], x) + theta[1])) + theta[3])) + theta[5] #Maybe ohne sigma

# Definitionen der beiden l funktionen
def l1(S, Y):
    N = 0
    for i in range(len(S)):
        N += (S[i][0] - Y[i][0]) ** 2
    return N


def l2(S, Y):
    N = 0
    for i in range(len(S)):
        N += Y[i][0] * np.log1p(softmax(i, S))
    return -N


# Berechnet den Gradient von f(theta, x) abgeleitet nach dem Baies b^(l)
def gradb(l, theta, x, gradTf, sigma):
    # diag(D_sigma(z)) * W^T * b preparen
    z = zt(theta, x, l, sigma)
    z = functionDecider(sigma + "D", z).T  # transponieren, damit man dann als Liste zugreifen kann
    z = np.diag(z[0])

    # Mit den indizes gucken
    W = theta[2 * l].T
    b = gradTf[2 * l + 1]

    return np.matmul(np.matmul(z, W), b)  # multipliziert diag(D_sigma(z)) * W^T * b


# Berechnet den Gradienten von l(f(theta, x), y) abgeleitet nach der Weight matrix W^(l)
def gradW(l, theta, x, y, gradTf, sigma, ell):
    b = gradTf[2*l-1]
    b = np.matmul(ellDecider(ell, theta, x, y, sigma), b.T).T  # Damit dass zu ell wird
    a = at(theta, x, l-1, sigma).T
    return np.matmul(b, a)


# Berechnet den Gradient von l(f(theta, x), y) abgeleitet nach theta. ell ist der Gradient von l
def gradTf(theta, x, y, sigma, ell):
    o = np.asarray([])  # ist ein Platzhalter damit beim hinzufuegen die Indizes sich nicht verschieben

    w3 = np.matmul(ellDecider(ell, theta, x, y, sigma).T, at(theta, x, 2, sigma).T)  # Schon zur komponente von grad theta l
    b3 = np.eye(3, 3)

    L = [o, o, o, o, w3, b3]
    L[3] = gradb(2, theta, x, L, sigma)
    L[2] = gradW(2, theta, x, y, L, sigma, ell)
    L[1] = gradb(1, theta, x, L, sigma)
    L[0] = gradW(1, theta, x, y, L, sigma, ell)

    # Haben dass davor nicht gemacht, weil wir das ja bei dem b ohne das bruachen
    L[1] = np.matmul(ellDecider(ell, theta, x, y, sigma), L[1].T).T
    L[3] = np.matmul(ellDecider(ell, theta, x, y, sigma), L[3].T).T
    L[5] = np.matmul(ellDecider(ell, theta, x, y, sigma), L[5].T).T

    return L

def ellDecider(func, theta, x, y, sigma):
    if func == "l1":
        return l1(forward(theta, x, sigma), y)
    if func == "Dl1":
        return gradl1(theta, x, y, sigma)
    if func == "l2":
        return l2(forward(theta, x, sigma), y)
    if func == "Dl2":
        return gradl2(theta, x, y, sigma)

# Ableitung wenn l gleich die Eukliedischen-Norm ist
def gradl1(theta, x, y, sigma):
    f = forward(theta, x, sigma)

    return np.asarray([[2 * (f[0][0] - y[0][0]), 2 * (f[1][0] - y[1][0]), 2 * (f[2][0] - y[2][0])]])


# Berechnet die ableitung wenn l gleich der empirische log-Entropie Funktion ist
def gradl2(theta, x, y, sigma):
    f = forward(theta, x, sigma)
    # Berechne zuerst die einzelnen Komponenten
    a = softmax(0, f)
    b = softmax(1, f)
    c = softmax(2, f)
    # Dann zusammen in einen Vektor schreiben und das mit dem y
    return np.asarray([[-y[0][0] + a, -y[1][0] + b, -y[2][0] + c]])


# berechnet Softmax
def softmax(i, f):
    x = np.scipy.special.expit(f[i][0])

    b = 0
    for k in range(len(f)):
        b += np.scipy.special.expit(f[k][0])

    return x / b

#Berechnet den Gradienten der Loss Funktion nach Theta abgeleitet

def gradLoss(theta, sigma, ell):
    e = [np.zeros((64, 64)), np.zeros((64, 1)), np.zeros((32, 64)), np.zeros((32, 1)), np.zeros((3, 32)), np.zeros((3, 1))] #Das Ergebnis
    sa = random.sample(mask, 10)
    for i in sa:
        l = gradTf(theta, vec(digits.images[i]), one_hot(digits.target[i]), sigma, ell)
        e[0] = np.add(e[0], l[0]) / len(sa)
        e[1] = np.add(e[1], l[1]) / len(sa)
        e[2] = np.add(e[2], l[2]) / len(sa)
        e[3] = np.add(e[3], l[3]) / len(sa)
        e[4] = np.add(e[4], l[4]) / len(sa)
        e[5] = np.add(e[5], l[5]) / len(sa)
    return e


#fuehrt das Gradienten Absteigsverfahren fuer eine Schrittweite a und i Iterationen aus
def graddescent(theta, a, n, sigma, ell):
    for j in range(n):
        theta[0] = np.add(theta[0], - a * gradLoss(theta, sigma, ell)[0])
        theta[1] = np.add(theta[1], - a * gradLoss(theta, sigma, ell)[1])
        theta[2] = np.add(theta[2], - a * gradLoss(theta, sigma, ell)[2])
        theta[3] = np.add(theta[3], - a * gradLoss(theta, sigma, ell)[3])
        theta[4] = np.add(theta[4], - a * gradLoss(theta, sigma, ell)[4])
        theta[5] = np.add(theta[5], - a * gradLoss(theta, sigma, ell)[5])
    return theta

def main(stepsize, gradIter, Epochen, sigma, ell, n):
    theta = Theta
    for i in range(Epochen):
        theta = graddescent(theta, stepsize, gradIter, sigma, ell)
    C_matrix = np.zeros((3,3))
    #print(theta)
    for i in range(n):
        print(forward(theta,vec(digits.images[mask[i]]),sigma))
        guess = maxindex(forward(theta,vec(digits.images[mask[i]]),sigma))
        actual = maxindex(one_hot(digits.target[mask[i]]))
        print(actual,guess)
        C_matrix[actual][guess] += 1
    return C_matrix

def maxindex(A):
    if A[0][0] > A[1][0] and A[0][0] > A[2][0]:
        return 0
    elif A[1][0] > A[0][0] and A[1][0] > A[2][0]:
        return 1
    elif A[2][0] > A[1][0] and A[2][0] > A[0][0]:
        return 2

print(main(0.1, 50, 70,"sm", "Dl1", len(mask)))

"""
Sigmoid und DL1: x durch 50 teilen Theta con 1 bis -1 und dann Stepsize 0.1 und gradIter = 50 Epchen = 70 
"""

