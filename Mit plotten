import matplotlib.pyplot as plt
import sklearn as sk
from sklearn.datasets import load_digits
import numpy as np


X = load_digits().images
Y = load_digits().target

TrainingI, TestI, TrainingT, TestT = sk.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)

mask = np.isin(TrainingT, [1,5,7])
testmask = np.isin(TestT, [1,5,7])

#print(TestI[5])
#print(TestI)

#Maske für unseren Datensatz

maskI = []
maskT = []
for i in range(len(TrainingI)):
    if mask[i]:
        maskI.append(TrainingI[i])
        maskT.append(TrainingT[i])

testmaskI = []
testmaskT = []
for i in range(len(TestT)):
    if testmask[i]:
        testmaskI.append(TestI[i])
        testmaskT.append(TestT[i])

#One-hot-Funktion
def one_hot(data):
    if data == 1:
        return np.array([[1],[0],[0]])
    elif data == 5:
        return np.array([[0],[1],[0]])
    elif data == 7:
        return np.array([[0],[0],[1]])

x = 1
y = -1
# Macht Theta mit random Werten zwischen 0 und 1
W1 = np.random.uniform(y , x, (64, 64))
b1 = np.random.uniform(y , x, (64, 1))
W2 = np.random.uniform(y , x, (32, 64))
b2 = np.random.uniform(y , x, (32, 1))
W3 = np.random.uniform(y , x, (3, 32))
b3 = np.random.uniform(y , x, (3, 1))

Theta = [W1, b1, W2, b2, W3, b3]

# Definition der Sigma Funktionen
def softplus(X):
    L = np.empty((len(X), 1))
    for i in range(len(X)):
        L[i][0] = np.log1p(np.exp(X[i][0]))
    return L


def sigmoid(X):
    L = np.empty((len(X), 1))
    for i in range(len(X)):
        L[i][0] = 1 / (1 + np.exp(-X[i][0]))
    return L


# Ableitungen der Sigma Funktionen
def d_softplus(X):
    return sigmoid(X)


def d_sigmoid(X):
    L = np.empty((len(X), 1))
    for i in range(len(X)):
        L[i][0] = np.exp(-X[i][0]) / ((1 + np.exp(-X[i][0])) ** 2)
    return L


def vec(A):
    L = []
    for i in range(0, len(A[0])):
        for j in range(0, len(A)):
            L.append([A[j][i]/50]) #Damit die zwischen 0 und 1 sind.
    return np.asarray(L)


def zunda(theta, x, sigma):
    z1 = np.matmul(theta[0], x) + theta[1]
    a1 = functionDecider(sigma, z1)
    z2 = np.matmul(theta[2], a1) + theta[3]
    a2 = functionDecider(sigma, z2)
    z3 = np.matmul(theta[4], a2) + theta[5]
    #a3 = functionDecider(sigma, z3) brauchen wir glaub ich nicht
    return [z1, a1, z2, a2, z3]

def functionDecider(func, X):
    if func == "sp":
        return softplus(X)
    if func == "spD":
        return d_softplus(X)
    if func == "sm":
        return sigmoid(X)
    if func == "smD":
        return d_sigmoid(X)

# Auswertung des neuronalen Netzes mit beliebigem theta
def forward(theta, x, sigma):
    return np.matmul(theta[4], functionDecider(sigma, np.matmul(theta[2], functionDecider(sigma, np.matmul(theta[0], x) + theta[1])) + theta[3])) + theta[5] #Maybe ohne sigma

# Definitionen der beiden l funktionen
def l1(S, Y):
    N = 0
    for i in range(len(S)):
        N += (S[i][0] - Y[i][0]) ** 2
    return N


def l2(S, Y):
    N = 0
    for i in range(len(S)):
        N += Y[i][0] * np.log1p(softmax(i, S))
    return -N


# Berechnet den Gradient von f(theta, x) abgeleitet nach dem Baies b^(l)
def gradb(l, theta, gradTf, sigma, z):
    # diag(D_sigma(z)) * W^T * b preparen

    z = functionDecider(sigma + "D", z).T  # transponieren, damit man dann als Liste zugreifen kann
    z = np.diag(z[0])

    # Mit den indizes gucken
    W = theta[2 * l].T
    b = gradTf[2 * l + 1]

    return np.matmul(np.matmul(z, W), b)  # multipliziert diag(D_sigma(z)) * W^T * b


# Berechnet den Gradienten von l(f(theta, x), y) abgeleitet nach der Weight matrix W^(l)
def gradW(l, gradTf, a):
    b = gradTf[2*l-1]
    a = a.T
    return np.matmul(b, a)


# Berechnet den Gradient von l(f(theta, x), y) abgeleitet nach theta. ell ist der Gradient von l
def gradTf(theta, x, y, sigma, ell):
    o = np.asarray([])  # ist ein Platzhalter damit beim hinzufuegen die Indizes sich nicht verschieben
    az = zunda(theta, x, sigma)

    f = az[4]

    w3 = np.matmul(ellDecider(ell, y, f).T, az[3].T)  # Schon zur komponente von grad theta l
    b3 = np.eye(3, 3)

    L = [o, o, o, o, w3, b3]
    L[3] = gradb(2, theta, L, sigma, az[2])
    L[1] = gradb(1, theta, L, sigma, az[0])

    L[1] = np.matmul(ellDecider(ell, y, f), L[1].T).T
    L[3] = np.matmul(ellDecider(ell, y, f), L[3].T).T
    L[5] = np.matmul(ellDecider(ell, y, f), L[5].T).T

    L[2] = gradW(2, L, az[1])
    L[0] = gradW(1, L, x)

    return L


def ellDecider(func, y, f):
    if func == "l1":
        return l1(f, y)
    if func == "Dl1":
        return gradl1(y, f)
    if func == "l2":
        return l2(f, y)
    if func == "Dl2":
        return gradl2(y, f)

# Ableitung wenn l gleich die Eukliedischen-Norm ist
def gradl1(y, f):
    return np.asarray([[2 * (f[0][0] - y[0][0]), 2 * (f[1][0] - y[1][0]), 2 * (f[2][0] - y[2][0])]])


# Berechnet die ableitung wenn l gleich der empirische log-Entropie Funktion ist
def gradl2(y, f):
    # Berechne zuerst die einzelnen Komponenten
    a = softmax(0, f)
    b = softmax(1, f)
    c = softmax(2, f)
    # Dann zusammen in einen Vektor schreiben und das mit dem y
    return np.asarray([[-y[0][0] + a, -y[1][0] + b, -y[2][0] + c]])


# berechnet Softmax
def softmax(i, f):
    x = np.exp(f[i][0])

    b = 0
    for k in range(len(f)):
        b += np.exp(f[k][0])

    return x / b

#Berechnet den Gradienten der Loss Funktion nach Theta abgeleitet
# Aufgabe 4 die Funktion die die Gelernten Parameter updatet.
def gradLoss(theta, sigma, ell):
    e = [np.zeros((64, 64)), np.zeros((64, 1)), np.zeros((32, 64)), np.zeros((32, 1)), np.zeros((3, 32)), np.zeros((3, 1))] #Das Ergebnis
    sa = np.random.randint(0, len(maskI), size=10)
    #print(sa)
    for i in sa:
        l = gradTf(theta, vec(maskI[i]), one_hot(maskT[i]), sigma, ell)
        m = len(sa)
        e[0] = np.add(e[0], l[0]) / m
        e[1] = np.add(e[1], l[1]) / m
        e[2] = np.add(e[2], l[2]) / m
        e[3] = np.add(e[3], l[3]) / m
        e[4] = np.add(e[4], l[4]) / m
        e[5] = np.add(e[5], l[5]) / m
    return e


#fuehrt das Gradienten Absteigsverfahren fuer eine Schrittweite a und i Iterationen aus
def graddescent(theta, a, n, sigma, ell):
    Tr = []
    Te = []
    for j in range(n):
        gL = gradLoss(theta, sigma, ell)
        theta[0] = np.add(theta[0], - a * gL[0])
        theta[1] = np.add(theta[1], - a * gL[1])
        theta[2] = np.add(theta[2], - a * gL[2])
        theta[3] = np.add(theta[3], - a * gL[3])
        theta[4] = np.add(theta[4], - a * gL[4])
        theta[5] = np.add(theta[5], - a * gL[5])

    for i in range(len(maskT)):
        t = guess(forward(theta, vec(maskI[i]), sigma), maskT[i])
        Tr.append(t)
    #print(Tr)

    for i in range(len(testmaskT)):
        t = guess(forward(theta, vec(testmaskI[i]), sigma), testmaskT[i])
        Te.append(t)

    return theta, Tr, Te


def guess(f, target):
    target = one_hot(target)
    if np.argmax(target.T[0]) == np.argmax(f.T[0]):
        return 1
    return 0


def train_full_bath(stepsize, gradIter, Epochen, sigma, ell):
    theta = Theta
    c = 0
    """
    LösungTrain = []
    for i in range(len(maskT)):
        LösungTrain.append(np.argmax(one_hot(maskT[i]), axis=1))

    LösungTest = []
    for i in range(len(testmaskT)):
        LösungTest.append(np.argmax(one_hot(testmaskT[i]), axis=1))
    """
    A = [1 for i in range(len(maskT))]
    B = [1 for i in range(len(testmaskT))]

    #print(A)

    LTr = []
    LTe = []

    for i in range(Epochen):
        theta, Tr, Te = graddescent(theta, stepsize, gradIter, sigma, ell)
        c += 1
        print(f"\rEpoche: {c}", end="", flush=True)

        """
        Hier muss man dann immer den Loss berechnen und sich die Werte Merken.
        Den Loss müssen wir für Test und Trainingssätze machen, also das alles in der For schleife hier.
        
        sklearn.metrics.accuracy_score: Damit berechnen wir den Fehler der Gemacht wurde also den Loss.
        Das nimmt zwei Listen mit Zahlen drin also zwei eindimensionale Listen. Man soll dann das Maximum in der Reihe nehmen
        """
        LTr.append(sk.metrics.accuracy_score(Tr, A))
        LTe.append(sk.metrics.accuracy_score(Te, B))

    # hier dann die plotten Funktion ausführen.
    plt.plot(LTr, label="Loss der Trainingsdaten", color="blue")
    plt.plot(LTe, label="Loss der Testdaten", color="red")
    plt.xlabel("Epochen")
    plt.ylabel("Loss")
    plt.title("Loss der Trainings und Test Daten")
    plt.legend()
    plt.show()

    C_matrix = np.zeros((3,3))
    #print(theta)
    for i in range(len(testmaskI)):
        #print(forward(theta,vec(digits.images[mask[i]]),sigma))
        guess = maxindex(forward(theta,vec(testmaskI[i]), sigma))
        actual = maxindex(one_hot(testmaskT[i]))
        #print(actual,guess)
        C_matrix[actual][guess] += 1
    print()
    print("Confusion-Matrix:")
    return C_matrix

def maxindex(A):
    if A[0][0] > A[1][0] and A[0][0] > A[2][0]:
        return 0
    elif A[1][0] > A[0][0] and A[1][0] > A[2][0]:
        return 1
    elif A[2][0] > A[1][0] and A[2][0] > A[0][0]:
        return 2

#sm und Dl1
print(train_full_bath(0.1, 40, 150,"sm", "Dl1"))

#sp und Dl1
#print(train_full_bath(0.03, 80, 150,"sp", "Dl1"))

#sm und Dl2
#print(train_full_bath(0.1, 40, 150,"sm", "Dl2"))

#sp und Dl2
#print(train_full_bath(0.5, 40, 150,"sp", "Dl2"))

"""
Bei x durch 50 Teilen
Theta: -1 bis 1
grad Iter: 150
Epochen: 150
Zeit: immer so 30 sekunden

Sigmoid und DL1: stepsize: 0.1  Fehler: 1 

Softplus und DL1: stepsize: 0.03  Fehler: 1 bis 3

Sigmoid und DL2: stepsize: 0.1  Fehler: 1 

Softplus und Dl2: stepsize: 0.5  Fehler: 1 bis 2
"""
